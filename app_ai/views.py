from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods, require_GET, require_POST
import json
from .git_client import GitHubWebhookClient, GitHubDataClient
from .schemas import is_valid_async_data_type, ASYNC_DATA_TYPES, success_response, error_response
from .sql_client import DatabaseClient
# from .service import process_webhook_event  # ä¸å†ä½¿ç”¨åŒæ­¥å¤„ç†
from .tasks.async_get import fetch_github_data_async
from .tasks.async_ollama import start_ollama_analysis, start_single_commit_analysis
from .tasks.async_push import start_push_task
from celery.result import AsyncResult
import logging

logger = logging.getLogger(__name__)


# ==================== Webhookå¤„ç† ====================

@csrf_exempt
@require_http_methods(["POST"])
def git_webhook(request):
    """GitHub webhookå¤„ç†å™¨ - å¼‚æ­¥ç‰ˆæœ¬"""
    github_client = GitHubWebhookClient()
    is_valid, error_response_data, payload = github_client.validate_webhook_request(request)
    
    if not is_valid:
        return error_response_data
    
    event_type = request.META.get('HTTP_X_GITHUB_EVENT', '')
    logger.info(f"å¤„ç†webhookäº‹ä»¶: {event_type}")
    
    # å¤„ç†pushäº‹ä»¶ï¼Œè‡ªåŠ¨è§¦å‘å¼‚æ­¥æ•°æ®è·å–å’Œåˆ†æ
    if event_type == 'push':
        try:
            # å¯åŠ¨å¼‚æ­¥GitHubæ•°æ®è·å–ä»»åŠ¡
            task = fetch_github_data_async.delay(
                data_type='recent_commits',
                params={
                    'branch': payload.get('ref', 'refs/heads/main').replace('refs/heads/', ''),
                    'limit': 10,
                    'include_diff': True
                }
            )
            
            logger.info(f"ğŸš€ Webhookè§¦å‘å¼‚æ­¥ä»»åŠ¡: {task.id}")
            
            return JsonResponse(success_response({
                'event_type': event_type,
                'message': 'Pushäº‹ä»¶å¤„ç†æˆåŠŸï¼Œå·²å¯åŠ¨å¼‚æ­¥æ•°æ®è·å–å’ŒAIåˆ†æ',
                'async_task_id': task.id,
                'repository': payload.get('repository', {}).get('full_name', 'unknown'),
                'branch': payload.get('ref', 'refs/heads/main').replace('refs/heads/', ''),
                'commits_count': len(payload.get('commits', [])),
                'check_url': f'/ai/task-status/{task.id}/'
            }))
            
        except Exception as e:
            logger.error(f"Webhookå¼‚æ­¥ä»»åŠ¡å¯åŠ¨å¤±è´¥: {e}")
            return JsonResponse(error_response(f'å¼‚æ­¥ä»»åŠ¡å¯åŠ¨å¤±è´¥: {str(e)}', 500), status=500)
    
    elif event_type == 'ping':
        return JsonResponse(success_response({
            'event_type': event_type,
            'message': 'Webhook pingäº‹ä»¶å¤„ç†æˆåŠŸ',
            'repository': payload.get('repository', {}).get('full_name', 'unknown')
        }))
    
    else:
        logger.info(f"å¿½ç•¥äº‹ä»¶ç±»å‹: {event_type}")
        return JsonResponse(success_response({
            'event_type': event_type,
            'message': f'äº‹ä»¶ç±»å‹ {event_type} å·²å¿½ç•¥'
        }))


# ==================== åŒæ­¥æ•°æ®æ¥å£ ====================

@require_GET
def get_github_data(request):
    """åŒæ­¥è·å–GitHubæ•°æ®æ¥å£"""
    data_type = request.GET.get('type', '').strip()
    if not data_type:
        return JsonResponse(error_response("Missing required parameter: type", 400), status=400)
    
    if not is_valid_async_data_type(data_type):
        return JsonResponse(error_response(
            f'Invalid data type. Must be one of: {", ".join(ASYNC_DATA_TYPES)}', 400
        ), status=400)
    
    # ç®€åŒ–å‚æ•°å¤„ç†ï¼Œåªè·å–åŸºæœ¬å‚æ•°
    params = {
        'branch': request.GET.get('branch', 'main'),
        'limit': int(request.GET.get('limit', 10)),
        'sha': request.GET.get('sha', ''),
        'include_diff': request.GET.get('include_diff', 'true').lower() == 'true'
    }
    
    data_client = GitHubDataClient()
    result = data_client.get_data(data_type, **params)
    
    if result.get('status') == 'error':
        return JsonResponse(error_response(
            result.get('error', 'Unknown error'), 500
        ), status=500)
    
    # è‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
    _save_data_to_database(data_type, result, data_client)
    
    return JsonResponse(success_response(result))


def _save_data_to_database(result, data_type, data_client):
    """ç»Ÿä¸€çš„æ•°æ®åº“ä¿å­˜é€»è¾‘"""
    if data_type == 'commit_details' and result.get('status') == 'success':
        _save_single_commit(result)
    elif data_type == 'recent_commits' and result.get('status') == 'success':
        _save_recent_commits_batch(result, data_client)


def _save_single_commit(result):
    """ä¿å­˜å•ä¸ªæäº¤åˆ°æ•°æ®åº“"""
    try:
        if 'commit_detail' in result and 'commit' in result['commit_detail']:
            commit_detail = result['commit_detail']['commit']
            github_data = {
                'sha': commit_detail['sha'],
                'commit': {
                    'author': {
                        'name': commit_detail['author']['name'],
                        'email': commit_detail['author']['email'],
                        'date': commit_detail['timestamp']['authored_date']
                    },
                    'message': commit_detail['message']
                },
                'author': {
                    'login': commit_detail['author']['username'],
                    'avatar_url': commit_detail['author'].get('avatar_url')
                },
                'html_url': commit_detail['urls']['html_url'],
                'url': commit_detail['urls']['api_url'],
                'stats': commit_detail.get('stats', {}),
                'files': commit_detail.get('files', []),
                'parents': commit_detail.get('parents', [])
            }
            
            success, message, commit_obj = DatabaseClient.save_commit_to_database(github_data)
            result['database_save'] = {
                'success': success,
                'message': message,
                'commit_saved': commit_obj.commit_sha[:8] if commit_obj else None
            }
            
    except Exception as e:
        result['database_save'] = {
            'success': False,
            'message': f'æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}',
            'commit_saved': None
        }


def _save_recent_commits_batch(result, data_client):
    """æ‰¹é‡ä¿å­˜æœ€è¿‘æäº¤åˆ°æ•°æ®åº“"""
    try:
        if 'commits_data' in result and 'commits' in result['commits_data']:
            commits = result['commits_data']['commits']
            saved_count = 0
            
            for commit in commits:
                try:
                    detail_result = data_client.get_data('commit_details', 
                                                       sha=commit['sha'], 
                                                       include_diff=True)
                    
                    if detail_result.get('status') == 'success':
                        commit_detail = detail_result['commit_detail']['commit']
                        github_data = {
                            'sha': commit_detail['sha'],
                            'commit': {
                                'author': {
                                    'name': commit_detail['author']['name'],
                                    'email': commit_detail['author']['email'],
                                    'date': commit_detail['timestamp']['authored_date']
                                },
                                'message': commit_detail['message']
                            },
                            'author': {
                                'login': commit_detail['author']['username'],
                                'avatar_url': commit_detail['author'].get('avatar_url')
                            },
                            'html_url': commit_detail['urls']['html_url'],
                            'url': commit_detail['urls']['api_url'],
                            'stats': commit_detail.get('stats', {}),
                            'files': commit_detail.get('files', []),
                            'parents': commit_detail.get('parents', []),
                            'patch': commit_detail.get('raw_patch', '')
                        }
                    else:
                        # ç®€åŒ–ç‰ˆæœ¬
                        github_data = {
                            'sha': commit['sha'],
                            'commit': {
                                'author': {
                                    'name': commit['author'],
                                    'email': 'unknown@example.com',
                                    'date': commit['date']
                                },
                                'message': commit['message']
                            },
                            'author': {'login': 'unknown', 'avatar_url': None},
                            'html_url': commit['url'],
                            'url': commit['url'],
                            'stats': {}, 'files': [], 'parents': [], 'patch': ''
                        }
                    
                    success, message, _ = DatabaseClient.save_commit_to_database(github_data)
                    if success:
                        saved_count += 1
                        
                except Exception as commit_error:
                    logger.error(f"å¤„ç†æäº¤ {commit['sha'][:8]} æ—¶å‡ºé”™: {commit_error}")
                    continue
            
            result['database_save'] = {
                'success': True,
                'message': f'æ‰¹é‡ä¿å­˜å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {saved_count}/{len(commits)} ä¸ªæäº¤',
                'saved_count': saved_count,
                'total_count': len(commits)
            }
            
    except Exception as e:
        result['database_save'] = {
            'success': False,
            'message': f'æ‰¹é‡ä¿å­˜å¤±è´¥: {str(e)}',
            'saved_count': 0
        }


# ==================== æ•°æ®åº“æŸ¥è¯¢æ¥å£ ====================
# æ³¨æ„ï¼šè¿™äº›æ¥å£åœ¨ urls.py ä¸­å®šä¹‰ä½†å¯èƒ½æœªè¢«å®é™…ä½¿ç”¨ï¼Œä¿ç•™ä»¥é˜²éœ€è¦


# ==================== å¼‚æ­¥æ¥å£ ====================

@require_POST
@csrf_exempt
def get_github_data_async(request):
    """å¼‚æ­¥è·å–GitHubæ•°æ®æ¥å£"""
    try:
        try:
            data = json.loads(request.body)
        except json.JSONDecodeError:
            return JsonResponse(error_response(
                'body', 'Invalid JSON format'
            ), status=400)
        
        data_type = data.get('type', '').strip()
        if not data_type:
            return JsonResponse(error_response("Missing required parameter: " + 'type'), status=400)
        
        if not is_valid_async_data_type(data_type):
            return JsonResponse(error_response(
                f'Invalid data type. Must be one of: {", ".join(ASYNC_DATA_TYPES)}', 400
            ), status=400)
        
        params = data.get('params', {})
        task = fetch_github_data_async.delay(data_type, **params)
        
        logger.info(f"å¯åŠ¨å¼‚æ­¥GitHubæ•°æ®è·å–ä»»åŠ¡: {task.id}, ç±»å‹: {data_type}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'status': 'pending',
            'message': f'å¼‚æ­¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨è·å– {data_type} æ•°æ®',
            'data_type': data_type,
            'params': params,
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except Exception as e:
        return JsonResponse(error_response(str(e)), status=500)


@require_GET
def get_task_status(request, task_id):
    """è·å–å¼‚æ­¥ä»»åŠ¡çŠ¶æ€"""
    try:
        task_result = AsyncResult(task_id)
        
        if task_result.state == 'PENDING':
            response = {
                'task_id': task_id,
                'status': 'pending',
                'message': 'ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­...',
                'progress': None
            }
        elif task_result.state == 'SUCCESS':
            response = {
                'task_id': task_id,
                'status': 'completed',
                'message': 'ä»»åŠ¡æ‰§è¡Œå®Œæˆ',
                'result': task_result.result
            }
        elif task_result.state == 'FAILURE':
            response = {
                'task_id': task_id,
                'status': 'failed',
                'message': 'ä»»åŠ¡æ‰§è¡Œå¤±è´¥',
                'error': str(task_result.info)
            }
        else:
            response = {
                'task_id': task_id,
                'status': task_result.state.lower(),
                'message': f'ä»»åŠ¡çŠ¶æ€: {task_result.state}',
                'info': str(task_result.info) if task_result.info else None
            }
        
        return JsonResponse(success_response(response))
        
    except Exception as e:
        return JsonResponse(error_response(str(e)), status=500)


@require_GET
def get_recent_commits_async_start(request):
    """å¿«é€Ÿå¯åŠ¨å¼‚æ­¥è·å–æœ€è¿‘æäº¤çš„ä»»åŠ¡"""
    try:
        limit = int(request.GET.get('limit', 10))
        include_details = request.GET.get('include_details', 'true').lower() == 'true'
        
        if limit < 1 or limit > 100:
            return JsonResponse(error_response(
                'limit', 'Must be between 1 and 100'
            ), status=400)
        
        task = fetch_github_data_async.delay('recent_commits', limit=limit, include_details=include_details)
        
        logger.info(f"å¯åŠ¨å¼‚æ­¥è·å–æœ€è¿‘æäº¤ä»»åŠ¡: {task.id}, limit={limit}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'status': 'pending',
            'message': f'å¼‚æ­¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨è·å–æœ€è¿‘ {limit} ä¸ªæäº¤',
            'data_type': 'recent_commits',
            'params': {'limit': limit, 'include_details': include_details},
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except ValueError:
        return JsonResponse(error_response(
            'limit', 'Must be an integer'
        ), status=400)
    except Exception as e:
        return JsonResponse(error_response(str(e)), status=500)


@require_GET
def get_commit_details_async_start(request):
    """å¿«é€Ÿå¯åŠ¨å¼‚æ­¥è·å–æäº¤è¯¦æƒ…çš„ä»»åŠ¡"""
    try:
        sha = request.GET.get('sha', '').strip()
        if not sha:
            return JsonResponse(error_response("Missing required parameter: " + 'sha'), status=400)
        
        include_diff = request.GET.get('include_diff', 'true').lower() == 'true'
        task = fetch_github_data_async.delay('commit_details', sha=sha, include_diff=include_diff)
        
        logger.info(f"å¯åŠ¨å¼‚æ­¥è·å–æäº¤è¯¦æƒ…ä»»åŠ¡: {task.id}, sha={sha[:8]}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'status': 'pending',
            'message': f'å¼‚æ­¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨è·å–æäº¤ {sha[:8]} çš„è¯¦æƒ…',
            'data_type': 'commit_details',
            'params': {'sha': sha, 'include_diff': include_diff},
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except Exception as e:
        return JsonResponse(error_response(str(e), 500), status=500)


# ==================== Ollamaåˆ†ææ¥å£ ====================

@require_POST
@csrf_exempt
def start_ollama_analysis_api(request):
    """å¯åŠ¨Ollamaåˆ†æä»»åŠ¡"""
    try:
        data = json.loads(request.body)
        commit_shas = data.get('commit_shas')  # å¯é€‰ï¼ŒæŒ‡å®šè¦åˆ†æçš„æäº¤SHAåˆ—è¡¨
        
        # å¯åŠ¨å¼‚æ­¥åˆ†æä»»åŠ¡
        task = start_ollama_analysis(commit_shas)
        
        logger.info(f"å¯åŠ¨Ollamaåˆ†æä»»åŠ¡: {task.id}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'message': f'Ollamaåˆ†æä»»åŠ¡å·²å¯åŠ¨',
            'target_commits': len(commit_shas) if commit_shas else 'æ‰€æœ‰æœªåˆ†ææäº¤',
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except json.JSONDecodeError:
        return JsonResponse(error_response('Invalid JSON payload', 400), status=400)
    except Exception as e:
        logger.error(f"å¯åŠ¨Ollamaåˆ†æä»»åŠ¡å¤±è´¥: {e}")
        return JsonResponse(error_response(str(e), 500), status=500)


@require_POST
@csrf_exempt
def analyze_single_commit_api(request):
    """åˆ†æå•ä¸ªæäº¤"""
    try:
        data = json.loads(request.body)
        commit_sha = data.get('commit_sha', '').strip()
        
        if not commit_sha:
            return JsonResponse(error_response("Missing required parameter: commit_sha", 400), status=400)
        
        # å¯åŠ¨å•ä¸ªæäº¤åˆ†æä»»åŠ¡
        task = start_single_commit_analysis(commit_sha)
        
        logger.info(f"å¯åŠ¨å•ä¸ªæäº¤åˆ†æä»»åŠ¡: {task.id}, æäº¤: {commit_sha[:8]}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'commit_sha': commit_sha,
            'message': f'å•ä¸ªæäº¤åˆ†æä»»åŠ¡å·²å¯åŠ¨',
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except json.JSONDecodeError:
        return JsonResponse(error_response('Invalid JSON payload', 400), status=400)
    except Exception as e:
        logger.error(f"å¯åŠ¨å•ä¸ªæäº¤åˆ†æå¤±è´¥: {e}")
        return JsonResponse(error_response(str(e), 500), status=500)


@require_GET
def get_unanalyzed_commits_api(request):
    """è·å–æœªåˆ†æçš„æäº¤åˆ—è¡¨"""
    try:
        limit = int(request.GET.get('limit', 20))
        if limit < 1 or limit > 100:
            return JsonResponse(error_response('limit must be between 1 and 100', 400), status=400)
        
        # è·å–æœªåˆ†æçš„æäº¤
        unanalyzed_commits = DatabaseClient.get_unanalyzed_commits(limit)
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        commits_data = []
        for commit in unanalyzed_commits:
            commits_data.append({
                'commit_sha': commit['commit_sha'],
                'short_sha': commit['commit_sha'][:8],
                'author_name': commit['author_name'],
                'commit_message': commit['commit_message'][:100] + '...' if len(commit['commit_message']) > 100 else commit['commit_message'],
                'commit_timestamp': commit['commit_timestamp'],
                'code_diff_length': len(commit.get('code_diff', '')),
                'created_at': commit['created_at']
            })
        
        return JsonResponse(success_response({
            'unanalyzed_commits': commits_data,
            'count': len(commits_data),
            'limit': limit,
            'message': f'æ‰¾åˆ° {len(commits_data)} ä¸ªæœªåˆ†æçš„æäº¤'
        }))
        
    except ValueError:
        return JsonResponse(error_response('limit must be an integer', 400), status=400)
    except Exception as e:
        logger.error(f"è·å–æœªåˆ†ææäº¤å¤±è´¥: {e}")
        return JsonResponse(error_response(str(e), 500), status=500)


# ==================== æ¨é€æ¥å£ ====================

@require_POST
@csrf_exempt
def start_push_task_api(request):
    """å¯åŠ¨æ¨é€ä»»åŠ¡"""
    try:
        data = json.loads(request.body)
        delay_seconds = data.get('delay_seconds', 3)  # é»˜è®¤3ç§’å»¶è¿Ÿ
        
        # éªŒè¯å»¶è¿Ÿæ—¶é—´
        if not isinstance(delay_seconds, (int, float)) or delay_seconds < 0:
            return JsonResponse(error_response('delay_seconds must be a non-negative number', 400), status=400)
        
        # å¯åŠ¨æ¨é€ä»»åŠ¡
        task = start_push_task(delay_seconds)
        
        logger.info(f"å¯åŠ¨æ¨é€ä»»åŠ¡: {task.id}")
        
        return JsonResponse(success_response({
            'task_id': task.id,
            'message': 'æ¨é€ä»»åŠ¡å·²å¯åŠ¨',
            'delay_seconds': delay_seconds,
            'check_url': f'/ai/task-status/{task.id}/'
        }))
        
    except json.JSONDecodeError:
        return JsonResponse(error_response('Invalid JSON payload', 400), status=400)
    except Exception as e:
        logger.error(f"å¯åŠ¨æ¨é€ä»»åŠ¡å¤±è´¥: {e}")
        return JsonResponse(error_response(str(e), 500), status=500)


@require_GET
def get_unpushed_commits_api(request):
    """è·å–æœªæ¨é€çš„åˆ†æç»“æœåˆ—è¡¨"""
    try:
        limit = int(request.GET.get('limit', 20))
        if limit < 1 or limit > 100:
            return JsonResponse(error_response('limit must be between 1 and 100', 400), status=400)
        
        # è·å–æœªæ¨é€çš„åˆ†æç»“æœ
        from app_ai.models import GitCommitAnalysis
        
        unpushed_records = GitCommitAnalysis.objects.filter(
            analysis_suggestion__isnull=False,
            is_push=0
        ).exclude(
            analysis_suggestion=''
        ).order_by('commit_timestamp')[:limit]
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        commits_data = []
        for record in unpushed_records:
            commits_data.append({
                'commit_sha': record.commit_sha,
                'short_sha': record.commit_sha[:8],
                'author_name': record.author_name,
                'commit_message': record.commit_message[:100] + '...' if len(record.commit_message) > 100 else record.commit_message,
                'commit_timestamp': record.commit_timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                'analysis_length': len(record.analysis_suggestion),
                'code_diff_length': len(record.code_diff) if record.code_diff else 0,
                'created_at': record.created_at.strftime('%Y-%m-%d %H:%M:%S')
            })
        
        return JsonResponse(success_response({
            'unpushed_commits': commits_data,
            'count': len(commits_data),
            'limit': limit,
            'message': f'æ‰¾åˆ° {len(commits_data)} ä¸ªæœªæ¨é€çš„åˆ†æç»“æœ'
        }))
        
    except ValueError:
        return JsonResponse(error_response('limit must be an integer', 400), status=400)
    except Exception as e:
        logger.error(f"è·å–æœªæ¨é€æäº¤å¤±è´¥: {e}")
        return JsonResponse(error_response(str(e), 500), status=500)